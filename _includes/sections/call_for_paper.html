<div class="container" id="i18_about_me">
    <div class="row m-b-lg">
        <div class="col-lg-12 text-center">
            <div class="navy-line"></div>
            <h1><span data-i18n="about_me.about_me">Call For Papers</span></h1>
            <!-- <p>Donec sed odio dui. Etiam porta sem malesuada magna mollis euismod.</p> -->
        </div>
    </div>
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 text-center m-t-lg m-b-lg wow zoomIn">
            <p><font size="4px"><span data-i18n="about_me.des"> We call for novel and unpublished work in the format of long papers (up to 8 pages) and short papers (up to 4 pages). Papers should follow the <a href="https://media.eventhosts.cc/Conferences/ICCV2025/ICCV2025-Author-Kit-Feb.zip" target="_blank"><span class="navy">ICCV proceedings style</span></a> and will be reviewed in a double-blind fashion. Submissions may be made to either of two tracks: (1) in-proceedings papers – long papers that will be published in the conference proceedings, and (2) out-of-proceedings papers – long or short papers that will not be included in the proceedings. <b>Note that according to the <a href="https://iccv.thecvf.com/Conferences/2025/AuthorGuidelines" target="_blank"><span class="navy">ICCV guidelines</span></a>, papers longer than four pages are considered published, even if they do not appear in the proceedings.</b> Selected long papers from both tracks will be invited for oral presentations; all accepted papers will be presented as posters. </span></font></p>
         </div>
         <div class="col-lg-8 col-lg-offset-2 m-t-lg m-b-lg wow zoomIn">
            <p><font size="4px" color="#000000"><span data-i18n="about_me.des">  Topics of interest include </span></font></p>
            <li><font size="3px">instance-level object classification, detection, segmentation, and pose estimation</font></li>
            <li><font size="3px">particular object (instance-level) and event retrieval</font></li>
            <li><font size="3px">personalized (instance-level) image and video generation</font></li>
            <li><font size="3px">cross-modal/multi-modal recognition at instance-level</font></li>
            <li><font size="3px">image matching, place recognition, video tracking</font></li>
            <li><font size="3px">other ILR+G applications or challenges</font></li>
            <li><font size="3px">ILR+G datasets and benchmarking</font></li> <br />
            <p><font size="3px" color="#000000"><span data-i18n="about_me.des">  The task of person re-identification clearly falls within our definition of ILR. Nevertheless, because of its social implications, we intentionally omit it from the list of topics. </span></font></p>
         </div>
         <div class="col-lg-8 col-lg-offset-2 m-t-lg m-b-lg wow zoomIn">
            <p><font size="4px" color="#000000"><span data-i18n="about_me.des">  Important Dates </span></font></p>
            <p><font size="3px" color="#000000"><span data-i18n="about_me.des">  In-proceedings papers </span></font></p>
            <li><font size="3px">Submission deadline: June 7, 2025</font></li>
            <li><font size="3px">Paper notification: June 21, 2025</font></li>
            <li><font size="3px">Camera-ready deadline: June 27, 2025</font></li><br />
            <p><font size="3px" color="#000000"><span data-i18n="about_me.des">  Out-of-proceedings papers </span></font></p>
            <li><font size="3px">Submission deadline: June 30, 2025</font></li>
            <li><font size="3px">Paper notification: July 18, 2025</font></li>
            <li><font size="3px">Camera-ready deadline: July 25, 2025</font></li><br />
        </div>
        <div class="col-lg-8 col-lg-offset-2 m-t-lg m-b-lg wow zoomIn">
            <p><font size="4px" color="#000000"><span data-i18n="about_me.des"> Questions? Please reach out to us at <a href= "mailto: ilr-workshop@googlegroups.com"> ilr-workshop@googlegroups.com </a> </span></font></p>
        </div>
    </div>
</div>
